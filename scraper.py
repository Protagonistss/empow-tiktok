"""ç®€å•çš„Apifyæ•°æ®çˆ¬å–è„šæœ¬

ä½¿ç”¨ApifyClientä¸»åŠ¨è¿è¡ŒActorå¹¶è·å–æ•°æ®ã€‚"""

import os
import json
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥ApifyClient
from apify_client import ApifyClient


class ApifyDataScraper:
    """Apifyæ•°æ®çˆ¬å–å™¨"""
    
    def __init__(self):
        self.api_token = os.getenv('APIFY_API_TOKEN')
        self.timeout = int(os.getenv('APIFY_TIMEOUT', 30))
        self.data_dir = Path(os.getenv('APP_DATA_DIR', './data'))
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(exist_ok=True)
        
        if not self.api_token:
            raise ValueError("APIFY_API_TOKEN æœªé…ç½®")
        
        # åˆå§‹åŒ–ApifyClient
        self.client = ApifyClient(self.api_token)
    
    def run_actor(self, actor_id: str, run_input: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è¿è¡ŒæŒ‡å®šçš„Actor"""
        try:
            print(f"ğŸš€ å¼€å§‹è¿è¡ŒActor: {actor_id}")
            print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {json.dumps(run_input, ensure_ascii=False, indent=2)}")
            
            # è¿è¡ŒActorå¹¶ç­‰å¾…å®Œæˆ
            run = self.client.actor(actor_id).call(run_input=run_input)
            
            print(f"âœ… Actorè¿è¡Œå®Œæˆ")
            print(f"ğŸ“‹ Run ID: {run.get('id')}")
            print(f"ğŸ“Š çŠ¶æ€: {run.get('status')}")
            print(f"â±ï¸ è¿è¡Œæ—¶é—´: {run.get('stats', {}).get('runTimeSecs', 0)} ç§’")
            
            return run
        except Exception as e:
            print(f"è¿è¡ŒActorå¤±è´¥: {e}")
            return None
    
    def get_dataset_data(self, dataset_id: str, limit: int = 1000, max_retries: int = 3) -> List[Dict[str, Any]]:
        """ä»æ•°æ®é›†è·å–æ•°æ®"""
        import time
        
        for retry in range(max_retries):
            try:
                print(f"ğŸ“ ä»æ•°æ®é›†è·å–æ•°æ®: {dataset_id} (å°è¯• {retry + 1}/{max_retries})")
                
                # ä½¿ç”¨ApifyClientè·å–æ•°æ®é›†æ•°æ®
                items = []
                for item in self.client.dataset(dataset_id).iterate_items():
                    items.append(item)
                    if len(items) >= limit:
                        break
                
                if items:
                    return items
                else:
                    print(f"âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œç­‰å¾… {(retry + 1) * 5} ç§’åé‡è¯•...")
                    time.sleep((retry + 1) * 5)  # é€’å¢ç­‰å¾…æ—¶é—´
                    
            except Exception as e:
                print(f"è·å–æ•°æ®é›†æ•°æ®å¤±è´¥ (å°è¯• {retry + 1}): {e}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 3)
        
        return []
    
    def save_data(self, data: List[Dict[str, Any]], filename: str = None) -> str:
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"apify_data_{timestamp}.json"
        
        filepath = self.data_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)
    
    def scrape_data(self, actor_id: str = "QwlnuM1ok9nxykQjF", 
                   start_url: str = "https://www.tiktok.com/shop/pdp/2025-2026-daily-planner-by-the-dailee-goal-setting-scheduling-to-do-lists/1731200046599410414?source=ecommerce_mall&enter_from=ecommerce_mall&enter_method=homepage_product_feed_savings",
                   max_items: int = 100,
                   use_test_mode: bool = False) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®çˆ¬å–"""
        if use_test_mode:
            print("ğŸ§ª ä½¿ç”¨æµ‹è¯•æ¨¡å¼ - çˆ¬å–ç½‘é¡µå†…å®¹...")
            # ä½¿ç”¨é€šç”¨ç½‘é¡µçˆ¬è™«è¿›è¡Œæµ‹è¯•
            actor_id = "apify/web-scraper"
            run_input = {
                "startUrls": [{"url": "https://apify.com/store"}],
                "linkSelector": "a[href]",
                "pageFunction": "async function pageFunction(context) { const { page, request } = context; const title = await page.title(); return { url: request.url, title }; }",
                "maxRequestsPerCrawl": min(max_items, 10)
            }
        else:
            print("ğŸš€ å¼€å§‹çˆ¬å–TikTokå•†å“æ•°æ®...")
            # å‡†å¤‡TikTok Actorè¾“å…¥å‚æ•°
            run_input = {
                "start_urls": [{"url": start_url}],
                "max_items": max_items,
            }
        
        # è¿è¡ŒActor
        run_result = self.run_actor(actor_id, run_input)
        if not run_result:
            return {"success": False, "message": "Actorè¿è¡Œå¤±è´¥"}
        
        # è·å–æ•°æ®é›†ID
        dataset_id = run_result.get('defaultDatasetId')
        if not dataset_id:
            return {"success": False, "message": "æœªæ‰¾åˆ°æ•°æ®é›†ID"}
        
        print(f"ğŸ“ æ•°æ®é›†ID: {dataset_id}")
        
        # è·å–æ•°æ® (å¢åŠ é‡è¯•æœºåˆ¶)
        data = self.get_dataset_data(dataset_id, max_items, max_retries=5)
        if not data:
            # å°è¯•ç›´æ¥ä»APIè·å–æ•°æ®é›†ä¿¡æ¯
            try:
                dataset_info = self.client.dataset(dataset_id).get()
                print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {dataset_info}")
                return {"success": False, "message": f"æœªè·å–åˆ°æ•°æ®ï¼Œæ•°æ®é›†å¯èƒ½ä¸ºç©ºã€‚æ•°æ®é›†ä¿¡æ¯: {dataset_info}"}
            except Exception as e:
                return {"success": False, "message": f"æœªè·å–åˆ°æ•°æ®ä¸”æ— æ³•è·å–æ•°æ®é›†ä¿¡æ¯: {e}"}
        
        print(f"ğŸ“¦ è·å–åˆ° {len(data)} æ¡æ•°æ®")
        
        # ä¿å­˜æ•°æ®
        filepath = self.save_data(data)
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        if data:
            print("\nğŸ“‹ æ•°æ®é¢„è§ˆ (å‰3æ¡):")
            for i, item in enumerate(data[:3], 1):
                print(f"  {i}. {json.dumps(item, ensure_ascii=False)[:100]}...")
        
        return {
            "success": True,
            "run_id": run_result.get('id'),
            "dataset_id": dataset_id,
            "data_count": len(data),
            "file_path": filepath,
            "run_info": run_result
        }


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    try:
        scraper = ApifyDataScraper()
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        use_test_mode = len(sys.argv) > 1 and sys.argv[1] == "--test"
        
        if use_test_mode:
            print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...")
            result = scraper.scrape_data(use_test_mode=True)
        else:
            print("ğŸš€ è¿è¡ŒTikTokçˆ¬å–æ¨¡å¼...")
            print("ğŸ’¡ æç¤º: ä½¿ç”¨ 'python scraper.py --test' è¿è¡Œæµ‹è¯•æ¨¡å¼")
            result = scraper.scrape_data()
        
        if result["success"]:
            print("\nâœ… æ•°æ®çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“Š å…±è·å– {result['data_count']} æ¡æ•°æ®")
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {result['file_path']}")
        else:
            print(f"\nâŒ æ•°æ®çˆ¬å–å¤±è´¥: {result['message']}")
            if not use_test_mode:
                print("\nğŸ’¡ å»ºè®®: å°è¯•è¿è¡Œæµ‹è¯•æ¨¡å¼ 'python scraper.py --test'")
    
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("\nğŸ’¡ å»ºè®®: æ£€æŸ¥API Tokené…ç½®å’Œç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    main()